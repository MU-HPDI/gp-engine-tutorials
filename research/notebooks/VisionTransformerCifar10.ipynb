{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6733bf32-0c66-4ff6-bd8e-c3f883e78f54",
   "metadata": {},
   "source": [
    "# Training a Vision Transformer on CIFAR-10 Using Kubernetes\n",
    "This notebook will  walk through training a Vision Transformer model on the CIFAR 10 dataset using Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca34c4-72d0-4d76-badc-e131db3bb55f",
   "metadata": {},
   "source": [
    "# Step 0: Prerequisites\n",
    "\n",
    "1. You must have an NRP account\n",
    "2. You must have been added to a Nautilus namespace\n",
    "3. You must have your NRP config in the `~/.kube` directory. There is a notebook to assist you [here](./NautilusConfigSetup.ipynb).\n",
    "4. You must have a PVC on the Nautilus cluster in your assigned namespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff8dae-411d-4a65-b22f-fd5fb00c0c8a",
   "metadata": {},
   "source": [
    "# Step 1: Creating a Train Script\n",
    "\n",
    "Our first step is to build a script to train and test our Vision Transformer. Let's begin by importing modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563861e6-51a8-4095-9bfc-125d6b6f75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# get number of jobs and epochs from the environment\n",
    "TORCH_NUM_JOBS = int(os.environ.get(\"TORCH_NUM_JOBS\", \"4\"))\n",
    "TORCH_NUM_EPOCHS = int(os.environ.get(\"TORCH_NUM_EPOCHS\", \"1\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84a746-5dc1-455b-b986-3167ebc4aa1a",
   "metadata": {},
   "source": [
    "Now, let's download the train partition and create our data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa649d-12ca-4f67-88f2-ba1ef74c2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train = CIFAR10(root=\"~/data\", download=True, train=True, transform=ViT_B_16_Weights.IMAGENET1K_V1.transforms())\n",
    "\n",
    "train_data_loader = DataLoader(cifar10_train,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               num_workers=TORCH_NUM_JOBS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d630b8-b55f-4040-8f78-33b2fc307f1b",
   "metadata": {},
   "source": [
    "Next is Model Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15404210-0524-484d-947c-e49a0e7308be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# set output neurons to Num Classes = 10\n",
    "model.heads[0] = nn.Linear(768, 10)\n",
    "\n",
    "# freeze the backbone\n",
    "model.encoder.requires_grad_(False)\n",
    "\n",
    "# create opt and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f50df4-f0e0-43ba-9b6b-3c88cce19a57",
   "metadata": {},
   "source": [
    "We can now train our model. Our job is going to utilize a GPU, so we can specify CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47169699-19db-49aa-9705-0ce5455d71c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(TORCH_NUM_EPOCHS):\n",
    "    print(\"*\" * 20 + f\"\\nEpoch {epoch+1} / {TORCH_NUM_EPOCHS}\")\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    \n",
    "    for i, (data, labels) in enumerate(train_data_loader):\n",
    "        data = data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_outputs = model(data)\n",
    "        \n",
    "        loss = loss_function(model_outputs.float(), labels.long())\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            raise RuntimeError(\"Loss reached NaN!\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "            \n",
    "        _, predictions = torch.max(model_outputs, 1)\n",
    "        epoch_correct += torch.sum(predictions == labels)        \n",
    "        epoch_loss += loss.item()\n",
    "        if i > 0 and (i % (len(train_data_loader) // 10) == 0 or i == 1):\n",
    "            print(f\"{i} / {len(train_data_loader)}\"\n",
    "                  f\"\\tLoss = {epoch_loss / i:.2f}\"\n",
    "                  f\"\\tAcc = {epoch_correct:d} / {i * train_data_loader.batch_size} \"\n",
    "                  f\"({epoch_correct / (i * train_data_loader.batch_size) * 100:.1f}%)\", flush=True)\n",
    "        \n",
    "    print(f\"Loss = {epoch_loss / len(train_data_loader):.4f}\")\n",
    "    print(f\"Train Acc = {epoch_correct / len(cifar10_train) * 100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d845d-7a44-435a-af19-53b40f29e935",
   "metadata": {},
   "source": [
    "Next, let's setup a test dataloader and run predictions on the test partition of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c198c8-ffef-4b1d-8cef-c2f4713b4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test = CIFAR10(root=\"~/data\", download=True, train=False, transform=ViT_B_16_Weights.IMAGENET1K_V1.transforms())\n",
    "\n",
    "test_data_loader = DataLoader(cifar10_test,\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_workers=TORCH_NUM_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3121d-c0da-4b3a-bf94-c0dabb7417e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    print(\"*\" * 20 + f\"\\nRunning Eval\")\n",
    "    for i, (data, lb) in enumerate(test_data_loader):\n",
    "        \n",
    "        model_outputs = model(data.cuda())\n",
    "\n",
    "        _, preds = torch.max(model_outputs, 1)\n",
    "\n",
    "        labels.extend(lb.numpy().tolist())\n",
    "        predictions.extend(preds.cpu().numpy().tolist())\n",
    "        if i > 0 and i % (len(test_data_loader) // 10) == 0:\n",
    "            print(f\"{i} / {len(test_data_loader)}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a66adc-9bc9-443d-b064-cced5319b75c",
   "metadata": {},
   "source": [
    "Finally, we can calculate statistics on the predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb0be1-00ba-4472-9d91-c01e36742764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as evaluate\n",
    "\n",
    "prec, rec, fscore, _ = evaluate(predictions, labels, average=\"macro\")\n",
    "\n",
    "print(\"*\" * 20 + f\"\"\"\\n\n",
    "Precision  \\t{prec*100:.2f}%\n",
    "Recall  \\t{rec*100:.2f}%\n",
    "F-1 Score \\t{fscore*100:.2f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aeecb6-eb52-45ff-9051-58a5aec377ca",
   "metadata": {},
   "source": [
    "# Step 2: Copying Our Script to the Cluster\n",
    "\n",
    "### Step 2A: Copy Code to a File\n",
    "The code we have written above has been copied to a script in the scripts directory named [ViTCifar10.py](../scripts/ViTCifar10.py).\n",
    "\n",
    "### Step 2B: Spawn Pod with PVC\n",
    "You now need to spawn a pod on the cluster with your peristent volume attached. You should have already updated the [pod_pvc.yml](../yaml/pod_pvc.yml) during the [SKLearn](./SKLearn.ipynb) exercise.\n",
    "\n",
    "\n",
    "Once you have updated the placeholder values, you can run the following cell to provision and start a pod on the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacdf03-ccf0-446b-979b-d87249fc35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl create -f ../yaml/pod_pvc.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c2685-ebd4-4c0b-9096-234a9c2849a5",
   "metadata": {},
   "source": [
    "### Step 2C: Copy the File to the PVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa80eb-197a-4cd2-9766-10a97e18d3cd",
   "metadata": {},
   "source": [
    "Run the following cell until your pod is `Running`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65099b-2b43-4053-a672-f0b2c67ecea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b69e3-26ac-48a0-a9cf-c9206f88f3db",
   "metadata": {},
   "source": [
    "Once your pod is running, we can copy our script to the PVC attached to the pod. Change `PODNAME` to your podname:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262a5b4-f50d-46ad-b6fc-c5a808488db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! kubectl cp ../scripts/ViTCifar10.py PODNAME:/data/ViTCifar10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98313238-1c76-4f0e-968f-31d2a0d7908b",
   "metadata": {},
   "source": [
    "We can check that our copy was successful with the `exec` subcommand in `kubectl`. Again, replace PODNAME with your pod's name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226db6e-8580-4750-8b58-4ce88e603bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl exec PODNAME -- cat /data/ViTCifar10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e538ff-9999-418f-a614-ec3d7fe54992",
   "metadata": {},
   "source": [
    "# Step 3: Building a Container Image\n",
    "\n",
    "The next step is to build and push a container to a container registry, but we can just use the same container image that is currently running this Jupyter instance, since it has all the dependencies we need:\n",
    "```\n",
    "gitlab-registry.nrp-nautilus.io/gp-engine/jupyter-stacks/bigdata-2023:latest\n",
    "```\n",
    "\n",
    "The `Dockerfile` for this image is in the docker directory of this repository [here](../docker/Dockerfile). However, in other contexts, you may need to write your own `Dockerfile` and build and push it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6945a14-c5b2-4040-8bb1-968eb439918b",
   "metadata": {},
   "source": [
    "# Step 4: Building the Job Specification YAML\n",
    "\n",
    "We now have everything we need to run our train and evaluation job. The final to-do item is to create a YAML Job Specification file. There is a template file for this in the repository [here](../yaml/sklearn_job_template.yml)\n",
    "\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {{ job_name }}\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "          - name: vit-train-container\n",
    "            image: gitlab-registry.nrp-nautilus.io/gp-engine/jupyter-stacks/bigdata-2023:latest\n",
    "            workingDir: /data\n",
    "            env:\n",
    "                - name: TORCH_NUM_JOBS\n",
    "                  value: \"{{ num_jobs }}\"\n",
    "                - name: TORCH_NUM_EPOCHS\n",
    "                  value: \"{{ num_epochs }}\"\n",
    "            command: [\"python3\", \"/data/ViTCifar10.py\"]\n",
    "            volumeMounts:\n",
    "                - name: {{ pvc_name }}\n",
    "                  mountPath: /data\n",
    "                - name: dshm\n",
    "                  mountPath: /dev/shm\n",
    "            resources:\n",
    "                limits:\n",
    "                  memory: 8Gi\n",
    "                  cpu: \"{{ num_jobs }}\"\n",
    "                  nvidia.com/gpu: 1\n",
    "                requests:\n",
    "                  memory: 8Gi\n",
    "                  cpu: \"{{ num_jobs }}\"    \n",
    "                  nvidia.com/gpu: 1\n",
    "      volumes:\n",
    "          - name: {{ pvc_name }}\n",
    "            persistentVolumeClaim:\n",
    "                claimName: {{ pvc_name }}\n",
    "          - name: dshm\n",
    "            emptyDir:\n",
    "              medium: Memory\n",
    "      affinity:\n",
    "        nodeAffinity:\n",
    "              preferredDuringSchedulingIgnoredDuringExecution:\n",
    "                  - weight: 1\n",
    "                    preference: \n",
    "                      matchExpressions:\n",
    "                        - key: nvidia.com/gpu.product\n",
    "                          operator: In\n",
    "                          values:\n",
    "                            - NVIDIA-A100-80GB-PCIe-MIG-1g.10gb\n",
    "      restartPolicy: Never      \n",
    "  backoffLimit: 1\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Let's use `jinja2` to fill in the missing values in our Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e8574-384a-45f8-bb27-9ac717f12f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "# read in the template\n",
    "with open('../yaml/vit_cifar10_job_template.yml') as file_:\n",
    "    template = Template(file_.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c5f31-ab8c-4c1a-bdb6-690627f796d0",
   "metadata": {},
   "source": [
    "Replace the arguments to the `render` function with the appropriate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f0b1d-db6d-4e2a-9f23-08628379763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the job spec\n",
    "job_spec = template.render(\n",
    "    job_name=\"JOBNAME\",\n",
    "    pvc_name=\"PVCNAME\",\n",
    "    num_jobs=8,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "# print the job spec\n",
    "print(job_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a778696-084e-4ecb-9451-dfd7c8c8d413",
   "metadata": {},
   "source": [
    "Now, let's save it to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e2598-0413-447e-a68b-ee6b8b876be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./vit_job.yml\", \"w\") as file:\n",
    "    file.write(job_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd558a2f-fbde-4b87-8896-d0f33a587858",
   "metadata": {},
   "source": [
    "# Step 5: Start the Job\n",
    "\n",
    "Run the cell below to start the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495c459-396c-43f3-9fad-7123634a6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl create -f ./vit_job.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496ff9a-3b90-478e-98bd-260f85639e5f",
   "metadata": {},
   "source": [
    "Run the cell below until your job moves to the `Complete` status. It will go through the stages of: `Pending`, `ContainerCreating`, and `Running`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97bb93-941e-4adb-9edf-abb45a6831be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a964b2d-7b09-471d-828b-2f750cc38823",
   "metadata": {},
   "source": [
    "**Note**: You can check the output as the job runs, once your pod moves to `Running`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85ea0e-55a7-4a58-b97f-51b71a31b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs PODNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7fc57-2f4b-4fe0-a958-0a6de7fbb34e",
   "metadata": {},
   "source": [
    "# Step 6: Review the Output of the Job\n",
    "\n",
    "As you can see in the output from Step 5, your job created a pod with the name of `job-ABCDE`. Let's check the output of that pod to see our accuracy. Change `PODNAME` below to the correct pod name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2bedc-c945-4d25-8580-ce53c77607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs --tail=5 PODNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126630e-4896-48a1-90c5-987735c28e67",
   "metadata": {},
   "source": [
    "# Step 7: Delete the Job and the Pod\n",
    "\n",
    "The final step is to delete the job we ran the pod we spawned. Please change `JOBNAME` and `PODNAME` below to the appropriate name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021adbcd-e917-4c16-a83f-a80374f8500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete job JOBNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002df753-9d96-4961-a7cd-7eac884892fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete pod PODNAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
